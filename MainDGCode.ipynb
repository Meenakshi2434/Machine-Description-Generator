{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Machine Description Generator\n",
    "Author: Meenakshi2434\n",
    "\n",
    "Description: \n",
    "- Generates technical equipment descriptions using LLM and PDF data\n",
    "- Processes Excel input files and PDF documentation\n",
    "- Integrates with FAISS vector database for context retrieval\n",
    "- Supports multilingual content translation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from googletrans import Translator, LANGUAGES\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration constants\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3b\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "CONFIDENCE_PDF = 1.0\n",
    "CONFIDENCE_LLM = 0.7\n",
    "\n",
    "# Status code mappings\n",
    "MACHINE_TYPE_MAPPING = {1: \"New\", 2: \"Used\"}\n",
    "CONDITION_MAPPING = {\n",
    "    1: \"In Stock\", 2: \"Running\", 3: \"Rebuilt\", \n",
    "    4: \"In Transit\", 5: \"In Production\", 6: \"Excellent\"\n",
    "}\n",
    "AVAILABILITY_MAPPING = {\n",
    "    1: \"Immediately\", 2: \"Less than 30 days\", \n",
    "    3: \"More than 30 days\", 4: \"Immediately from stock\"\n",
    "}\n",
    "\n",
    "def is_english(text: str) -> bool:\n",
    "    \"\"\"Robust language detection for English content\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return True\n",
    "    try:\n",
    "        cleaned = ' '.join(text.split())\n",
    "        return detect(cleaned) == 'en' if len(cleaned) >= 10 else True\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def translate_to_english(text: str) -> str:\n",
    "    \"\"\"Translate non-English text to English with chunk processing\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return text\n",
    "    translator = Translator()\n",
    "    chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]\n",
    "    translated = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            if not is_english(chunk):\n",
    "                translated.append(translator.translate(chunk, dest='en').text)\n",
    "            else:\n",
    "                translated.append(chunk)\n",
    "        except Exception:\n",
    "            translated.append(chunk)\n",
    "    return ' '.join(translated)\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load pre-trained LLM with optimized settings\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Model loading failed: {e}\")\n",
    "\n",
    "def clean_value(value) -> str:\n",
    "    \"\"\"Standardize and clean input values\"\"\"\n",
    "    if pd.isna(value) or (isinstance(value, str) and value.lower() == 'nan'):\n",
    "        return None\n",
    "    return str(value).strip()\n",
    "\n",
    "def parse_pdf_and_create_embeddings(folder_path: str) -> dict:\n",
    "    \"\"\"Extract and process PDF content into structured data\"\"\"\n",
    "    pdf_data = {}\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Warning: PDF folder {folder_path} not found\")\n",
    "        return pdf_data\n",
    "\n",
    "    for manufacturer in os.listdir(folder_path):\n",
    "        mfg_path = os.path.join(folder_path, manufacturer)\n",
    "        if not os.path.isdir(mfg_path):\n",
    "            continue\n",
    "        for model in os.listdir(mfg_path):\n",
    "            model_path = os.path.join(mfg_path, model)\n",
    "            if not os.path.isdir(model_path):\n",
    "                continue\n",
    "            combined_text = []\n",
    "            for pdf_file in os.listdir(model_path):\n",
    "                if pdf_file.lower().endswith('.pdf'):\n",
    "                    pdf_path = os.path.join(model_path, pdf_file)\n",
    "                    text = extract_text_from_pdf(pdf_path)\n",
    "                    if text:\n",
    "                        combined_text.append(preprocess_text(text))\n",
    "            if combined_text:\n",
    "                pdf_data[(manufacturer, model)] = \" \".join(combined_text)\n",
    "    return pdf_data\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text content from PDF documents\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        return \" \".join(page.extract_text() for page in reader.pages)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text content for processing\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "    text = re.sub(r'\\d+/\\d+', '', text)\n",
    "    return translate_to_english(text) if not is_english(text) else text\n",
    "\n",
    "def create_vector_store(pdf_data: dict):\n",
    "    \"\"\"Create FAISS vector store for semantic search\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for (mfg, model), text in pdf_data.items():\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        documents.extend(chunks)\n",
    "        metadatas.extend([{\"manufacturer\": mfg, \"model\": model}] * len(chunks))\n",
    "        \n",
    "    return FAISS.from_texts(documents, embeddings, metadatas=metadatas)\n",
    "\n",
    "def map_status_codes(row, manufacturer_mapping, model_mapping):\n",
    "    \"\"\"\n",
    "    Maps numeric codes to their corresponding text values using the mapping dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from the input DataFrame\n",
    "        manufacturer_mapping (pd.DataFrame): DataFrame containing manufacturer code to name mappings\n",
    "        model_mapping (pd.DataFrame): DataFrame containing model code to name mappings\n",
    "        \n",
    "    Returns:\n",
    "        dict: Mapped values or None if mapping fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean input values\n",
    "        mfg_code = clean_value(row.get('ManufacturerId'))\n",
    "        model_code = clean_value(row.get('MachineModelId'))\n",
    "        machine_type = clean_value(row.get('MachineTypeId'))\n",
    "        condition = clean_value(row.get('MachineConditionId'))\n",
    "        availability = clean_value(row.get('MachineAvailabilityId'))\n",
    "        location = clean_value(row.get('Location'))\n",
    "        year = clean_value(row.get('Year'))\n",
    "        description = clean_value(row.get('Description'))\n",
    "        \n",
    "        # Validate required fields\n",
    "        if not all([mfg_code, model_code]):\n",
    "            print(f\"Missing required codes for row: {row.name}\")\n",
    "            return None\n",
    "            \n",
    "        # Map manufacturer\n",
    "        try:\n",
    "            manufacturer = manufacturer_mapping.loc[\n",
    "                manufacturer_mapping['Id'] == int(mfg_code), \n",
    "                'Name'\n",
    "            ].iloc[0]\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"Invalid manufacturer code: {mfg_code}\")\n",
    "            return None\n",
    "            \n",
    "        # Map model\n",
    "        try:\n",
    "            model = model_mapping.loc[\n",
    "                model_mapping['MachineModelId'] == int(model_code), \n",
    "                'ModelName'\n",
    "            ].iloc[0]\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"Invalid model code: {model_code}\")\n",
    "            return None\n",
    "            \n",
    "        # Map other fields using the global mapping dictionaries\n",
    "        mapped_data = {\n",
    "            'ManufacturerId': manufacturer,\n",
    "            'MachineModelId': model,\n",
    "            'MachineTypeId': MACHINE_TYPE_MAPPING.get(int(machine_type)) if machine_type else 'Unknown',\n",
    "            'MachineConditionId': CONDITION_MAPPING.get(int(condition)) if condition else 'Unknown',\n",
    "            'MachineAvailabilityId': AVAILABILITY_MAPPING.get(int(availability)) if availability else 'Unknown',\n",
    "            'Location': location or 'Unknown',\n",
    "            'Year': year or 'Unknown',\n",
    "            'Description': description or 'No description available'\n",
    "        }\n",
    "        \n",
    "        return mapped_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error mapping status codes: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_relevant_context(vector_store, manufacturer, model, k=3):\n",
    "    \"\"\"Retrieve relevant context from vector store with improved filtering.\"\"\"\n",
    "    query = f\"{manufacturer} {model} technical specifications and features\"\n",
    "    docs = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    # Combine docs but avoid repetition\n",
    "    unique_content = set()\n",
    "    filtered_content = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        sentences = doc.page_content.split('.')\n",
    "        for sentence in sentences:\n",
    "            cleaned = sentence.strip()\n",
    "            if cleaned and cleaned not in unique_content:\n",
    "                unique_content.add(cleaned)\n",
    "                filtered_content.append(cleaned)\n",
    "    \n",
    "    return \". \".join(filtered_content)\n",
    "\n",
    "template = \"\"\"The {MachineConditionId} {MachineTypeId} machine, Model {MachineModelId} manufactured by {ManufacturerId}, is located in {Location}. This {Year} model is currently {MachineAvailabilityId} available. {context}\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt(row, manufacturer_mapping, model_mapping, vector_store, user_description=None):\n",
    "    \"\"\"\n",
    "    Create a prompt for description generation based on row data.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): Input data row\n",
    "        manufacturer_mapping (pd.DataFrame): Manufacturer mapping data\n",
    "        model_mapping (pd.DataFrame): Model mapping data\n",
    "        vector_store: FAISS vector store\n",
    "        user_description (str, optional): Additional user-provided description\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated prompt or None if mapping fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Map the status codes first\n",
    "        mapped_data = map_status_codes(row, manufacturer_mapping, model_mapping)\n",
    "        if not mapped_data:\n",
    "            return None\n",
    "            \n",
    "        # Get relevant context from vector store\n",
    "        context = get_relevant_context(\n",
    "            vector_store,\n",
    "            mapped_data['ManufacturerId'],\n",
    "            mapped_data['MachineModelId']\n",
    "        )\n",
    "        \n",
    "        # Format the template with the mapped data\n",
    "        prompt = template.format(\n",
    "            **mapped_data,\n",
    "            context=context,\n",
    "            location=mapped_data['Location'],\n",
    "            description=mapped_data['Description']\n",
    "        )\n",
    "        \n",
    "        # Add the user-provided description to the prompt, if available\n",
    "        if user_description:\n",
    "            prompt += f\"\\n\\nAdditional Details:\\n{user_description}\"\n",
    "        \n",
    "        return prompt\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating prompt: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def clean_generated_text(text):\n",
    "    \"\"\"Enhanced cleaning of generated text with improved error handling.\"\"\"\n",
    "    if not text:\n",
    "        return \"Error: Empty generated text.\"\n",
    "    \n",
    "    try:\n",
    "        # Remove unwanted introductory phrases\n",
    "        text = re.sub(r'You are a technical writer.*?\\n\\n', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'Create a detailed, professional description.*?Response must be in English only.', '', text, flags=re.DOTALL)\n",
    "        \n",
    "        # Clean up formatting\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', text)\n",
    "        \n",
    "        # Ensure complete sentences\n",
    "        if not text.endswith(('.', '!', '?')):\n",
    "            text = text.rstrip() + '.'\n",
    "        \n",
    "        # Verify English content and length\n",
    "        if not is_english(text):\n",
    "            translated = translate_to_english(text)\n",
    "            if translated and len(translated.split()) >= 3:\n",
    "                return translated\n",
    "            return \"Error: Unable to generate valid English description.\"\n",
    "        \n",
    "        # Check for minimum meaningful content\n",
    "        if len(text.split()) < 3:\n",
    "            return \"Error: Generated description too short.\"\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Text cleaning error: {str(e)}\")\n",
    "        return \"Error: Text cleaning failed.\"\n",
    "\n",
    "def generate_description(model, tokenizer, prompt: str) -> str:\n",
    "    \"\"\"Generate equipment descriptions using LLM\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", \n",
    "                         truncation=True, max_length=2048).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return clean_generated_text(raw_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return \"Error generating description\"\n",
    "\n",
    "def process_excel(input_file: str, output_file: str, pdf_folder: str):\n",
    "    \"\"\"Main processing pipeline for Excel files\"\"\"\n",
    "    try:\n",
    "        model, tokenizer = load_model()\n",
    "        pdf_data = parse_pdf_and_create_embeddings(pdf_folder)\n",
    "        vector_store = create_vector_store(pdf_data)\n",
    "        df = pd.read_excel(input_file)\n",
    "        \n",
    "        df['Generated_Description'] = ''\n",
    "        df['Source'] = ''\n",
    "        df['Confidence_Score'] = 0.0\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            mfg = clean_value(row['ManufacturerId'])\n",
    "            model_code = clean_value(row['MachineModelId'])\n",
    "            \n",
    "            if (mfg, model_code) in pdf_data:\n",
    "                context = pdf_data[(mfg, model_code)]\n",
    "                desc = generate_description(model, tokenizer, context)\n",
    "                df.at[idx, 'Generated_Description'] = desc\n",
    "                df.at[idx, 'Source'] = 'PDF'\n",
    "                df.at[idx, 'Confidence_Score'] = CONFIDENCE_PDF\n",
    "            else:\n",
    "                prompt = create_prompt(row, vector_store)\n",
    "                if prompt:\n",
    "                    desc = generate_description(model, tokenizer, prompt)\n",
    "                    df.at[idx, 'Generated_Description'] = desc\n",
    "                    df.at[idx, 'Source'] = 'LLM'\n",
    "                    df.at[idx, 'Confidence_Score'] = CONFIDENCE_LLM\n",
    "        \n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"Processing complete: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Processing failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_excel(\n",
    "        input_file=\"data/input/MD_data.xlsx\",\n",
    "        output_file=\"output/descriptions.xlsx\",\n",
    "        pdf_folder=\"data/pdfs\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
