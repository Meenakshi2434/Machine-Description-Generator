{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain-community\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall googletrans\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install --upgrade httpcore httpx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configuration constants\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3b\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "CONFIDENCE_PDF = 1.0\n",
    "CONFIDENCE_LLM = 0.7\n",
    "\n",
    "# Status code mappings\n",
    "MACHINE_TYPE_MAPPING = {1: \"New\", 2: \"Used\"}\n",
    "CONDITION_MAPPING = {\n",
    "    1: \"In Stock\", 2: \"Running\", 3: \"Rebuilt\", \n",
    "    4: \"In Transit\", 5: \"In Production\", 6: \"Excellent\"\n",
    "}\n",
    "AVAILABILITY_MAPPING = {\n",
    "    1: \"Immediately\", 2: \"Less than 30 days\", \n",
    "    3: \"More than 30 days\", 4: \"Immediately from stock\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"Check if text is primarily in English with robust error handling.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return True  # Return True for empty or non-string input\n",
    "    try:\n",
    "        cleaned_text = ' '.join(text.split())  # Remove extra spaces\n",
    "        if len(cleaned_text) < 10:  # If text is too short\n",
    "            return True\n",
    "        return detect(cleaned_text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return True  # Default to True on detection failure\n",
    "    except Exception as e:\n",
    "        print(f\"Language detection error: {str(e)}\")\n",
    "        return True\n",
    "\n",
    "def translate_to_english(text: str) -> str:\n",
    "    \"\"\"Translate non-English text to English with chunk processing\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return text\n",
    "    translator = GoogleTranslator(source='auto', target='en')\n",
    "    chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]\n",
    "    translated = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            if not is_english(chunk):\n",
    "                translated.append(translator.translate(chunk))\n",
    "            else:\n",
    "                translated.append(chunk)\n",
    "        except Exception:\n",
    "            translated.append(chunk)\n",
    "    return ' '.join(translated)\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the model and tokenizer with error handling.\"\"\"\n",
    "    try:\n",
    "        model_name = \"meta-llama/Llama-3.2-3b\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "def clean_value(value):\n",
    "    \"\"\"Enhanced value cleaning with type checking.\"\"\"\n",
    "    if isinstance(value, (float, int)):\n",
    "        return str(value) if not pd.isna(value) else None\n",
    "    if not value or pd.isna(value) or str(value).lower() == 'nan':\n",
    "        return None\n",
    "    return str(value).strip()\n",
    "\n",
    "def parse_pdf_and_create_embeddings(folder_path):\n",
    "    \"\"\"Parse PDFs from a folder structure and extract text content.\"\"\"\n",
    "    pdf_data = {}\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Warning: PDF folder path {folder_path} does not exist\")\n",
    "        return pdf_data\n",
    "    try:\n",
    "        for manufacturer_folder in os.listdir(folder_path):\n",
    "            manufacturer_path = os.path.join(folder_path, manufacturer_folder)\n",
    "            if not os.path.isdir(manufacturer_path):\n",
    "                continue\n",
    "            for model_folder in os.listdir(manufacturer_path):\n",
    "                model_path = os.path.join(manufacturer_path, model_folder)\n",
    "                if not os.path.isdir(model_path):\n",
    "                    continue\n",
    "                combined_text = []\n",
    "                for pdf_file in os.listdir(model_path):\n",
    "                    if not pdf_file.lower().endswith('.pdf'):\n",
    "                        continue\n",
    "                    pdf_path = os.path.join(model_path, pdf_file)\n",
    "                    try:\n",
    "                        text = extract_text_from_pdf(pdf_path)\n",
    "                        if text:\n",
    "                            cleaned_text = preprocess_text(text)\n",
    "                            combined_text.append(cleaned_text)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing PDF {pdf_file}: {str(e)}\")\n",
    "                        continue\n",
    "                if combined_text:\n",
    "                    pdf_data[(manufacturer_folder, model_folder)] = \"\\n\".join(combined_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF folder: {str(e)}\")\n",
    "    return pdf_data\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a single PDF file.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = []\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text.append(page_text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting text from page in {pdf_path}: {str(e)}\")\n",
    "                continue\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhanced text preprocessing with language detection and translation.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    try:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "        text = text.replace('\\n', ' ').strip()\n",
    "        text = ' '.join(text.split())\n",
    "        text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "        text = re.sub(r'\\d+/\\d+', '', text)\n",
    "        if not is_english(text):\n",
    "            text = translate_to_english(text)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Text preprocessing error: {str(e)}\")\n",
    "        return text\n",
    "\n",
    "def validate_pdf_structure(folder_path):\n",
    "    \"\"\"Validate the PDF folder structure.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: PDF folder path {folder_path} does not exist\")\n",
    "        return False\n",
    "    valid_structure = True\n",
    "    manufacturer_count = 0\n",
    "    for manufacturer in os.listdir(folder_path):\n",
    "        manufacturer_path = os.path.join(folder_path, manufacturer)\n",
    "        if not os.path.isdir(manufacturer_path):\n",
    "            continue\n",
    "        manufacturer_count += 1\n",
    "        for model in os.listdir(manufacturer_path):\n",
    "            model_path = os.path.join(manufacturer_path, model)\n",
    "            if not os.path.isdir(model_path):\n",
    "                continue\n",
    "            pdf_count = sum(1 for f in os.listdir(model_path) if f.lower().endswith('.pdf'))\n",
    "            if pdf_count == 0:\n",
    "                print(f\"Warning: No PDFs found in {manufacturer}/{model}/\")\n",
    "                valid_structure = False\n",
    "    if manufacturer_count == 0:\n",
    "        print(\"Error: No manufacturer folders found\")\n",
    "        return False\n",
    "    print(f\"Found {manufacturer_count} manufacturers with PDF data\")\n",
    "    return valid_structure\n",
    "\n",
    "def create_vector_store(pdf_data):\n",
    "    \"\"\"Create FAISS vector store from PDF data.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    for (manufacturer, model), text in pdf_data.items():\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        documents.extend(chunks)\n",
    "        metadatas.extend([{\"manufacturer\": manufacturer, \"model\": model}] * len(chunks))\n",
    "    return FAISS.from_texts(documents, embeddings, metadatas=metadatas)\n",
    "\n",
    "def map_status_codes(row, manufacturer_mapping, model_mapping):\n",
    "    \"\"\"Maps numeric codes to their corresponding text values.\"\"\"\n",
    "    try:\n",
    "        mfg_code = clean_value(row.get('ManufacturerId'))\n",
    "        model_code = clean_value(row.get('MachineModelId'))\n",
    "        machine_type = clean_value(row.get('MachineTypeId'))\n",
    "        condition = clean_value(row.get('MachineConditionId'))\n",
    "        availability = clean_value(row.get('MachineAvailabilityId'))\n",
    "        location = clean_value(row.get('Location'))\n",
    "        year = clean_value(row.get('Year'))\n",
    "        description = clean_value(row.get('Description'))\n",
    "        if not all([mfg_code, model_code]):\n",
    "            print(f\"Missing required codes for row: {row.name}\")\n",
    "            return None\n",
    "        try:\n",
    "            manufacturer = manufacturer_mapping.loc[\n",
    "                manufacturer_mapping['Id'] == int(mfg_code), \n",
    "                'Name'\n",
    "            ].iloc[0]\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"Invalid manufacturer code: {mfg_code}\")\n",
    "            return None\n",
    "        try:\n",
    "            model = model_mapping.loc[\n",
    "                model_mapping['MachineModelId'] == int(model_code), \n",
    "                'ModelName'\n",
    "            ].iloc[0]\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"Invalid model code: {model_code}\")\n",
    "            return None\n",
    "        mapped_data = {\n",
    "            'ManufacturerId': manufacturer,\n",
    "            'MachineModelId': model,\n",
    "            'MachineTypeId': MACHINE_TYPE_MAPPING.get(int(machine_type)) if machine_type else 'Unknown',\n",
    "            'MachineConditionId': CONDITION_MAPPING.get(int(condition)) if condition else 'Unknown',\n",
    "            'MachineAvailabilityId': AVAILABILITY_MAPPING.get(int(availability)) if availability else 'Unknown',\n",
    "            'Location': location or 'Unknown',\n",
    "            'Year': year or 'Unknown',\n",
    "            'Description': description or 'No description available'\n",
    "        }\n",
    "        return mapped_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error mapping status codes: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_relevant_context(vector_store, manufacturer, model, k=3):\n",
    "    \"\"\"Retrieve relevant context from vector store.\"\"\"\n",
    "    query = f\"{manufacturer} {model} technical specifications and features\"\n",
    "    docs = vector_store.similarity_search(query, k=k)\n",
    "    unique_content = set()\n",
    "    filtered_content = []\n",
    "    for doc in docs:\n",
    "        sentences = doc.page_content.split('.')\n",
    "        for sentence in sentences:\n",
    "            cleaned = sentence.strip()\n",
    "            if cleaned and cleaned not in unique_content:\n",
    "                unique_content.add(cleaned)\n",
    "                filtered_content.append(cleaned)\n",
    "    return \". \".join(filtered_content)\n",
    "\n",
    "template = \"\"\"The {MachineConditionId} {MachineTypeId} machine, Model {MachineModelId} manufactured by {ManufacturerId}, is located in {Location}. This {Year} model is currently {MachineAvailabilityId} available. {context}\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt(row, manufacturer_mapping, model_mapping, vector_store, user_description=None):\n",
    "    \"\"\"\n",
    "    Create a prompt for description generation based on row data.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): Input data row\n",
    "        manufacturer_mapping (pd.DataFrame): Manufacturer mapping data\n",
    "        model_mapping (pd.DataFrame): Model mapping data\n",
    "        vector_store: FAISS vector store\n",
    "        user_description (str, optional): Additional user-provided description\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated prompt or None if mapping fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Map the status codes first\n",
    "        mapped_data = map_status_codes(row, manufacturer_mapping, model_mapping)\n",
    "        if not mapped_data:\n",
    "            return None\n",
    "            \n",
    "        # Get relevant context from vector store\n",
    "        context = get_relevant_context(\n",
    "            vector_store,\n",
    "            mapped_data['ManufacturerId'],\n",
    "            mapped_data['MachineModelId']\n",
    "        )\n",
    "        \n",
    "        # Format the template with the mapped data\n",
    "        prompt = template.format(\n",
    "            **mapped_data,\n",
    "            context=context,\n",
    "            location=mapped_data['Location'],\n",
    "            description=mapped_data['Description']\n",
    "        )\n",
    "        \n",
    "        # Add the user-provided description to the prompt, if available\n",
    "        if user_description:\n",
    "            prompt += f\"\\n\\nAdditional Details:\\n{user_description}\"\n",
    "        \n",
    "        return prompt\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating prompt: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_excel(input_file, output_file, manufacturer_mapping_file, model_mapping_file, pdf_folder, user_description=None):\n",
    "    \"\"\"Process Excel files and generate descriptions.\"\"\"\n",
    "    try:\n",
    "        manufacturer_mapping = pd.read_csv(manufacturer_mapping_file)\n",
    "        model_mapping = pd.read_excel(model_mapping_file)\n",
    "        df = pd.read_excel(input_file)\n",
    "        model, tokenizer = load_model()\n",
    "        pdf_data = parse_pdf_and_create_embeddings(pdf_folder)\n",
    "        vector_store = create_vector_store(pdf_data)\n",
    "        df['Generated_Description'] = ''\n",
    "        df['Source'] = ''\n",
    "        df['Confidence_Score'] = 0.0\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            manufacturer = clean_value(row.get('ManufacturerId'))\n",
    "            model_code = clean_value(row.get('MachineModelId'))\n",
    "            pdf_source = (manufacturer, model_code) in pdf_data\n",
    "            if pdf_source:\n",
    "                pdf_text = pdf_data[(manufacturer, model_code)]\n",
    "                description = generate_description(model, tokenizer, pdf_text)\n",
    "                df.at[idx, 'Generated_Description'] = description\n",
    "                df.at[idx, 'Source'] = 'PDF'\n",
    "                df.at[idx, 'Confidence_Score'] = 1.0\n",
    "            else:\n",
    "                prompt = create_prompt(row, manufacturer_mapping, model_mapping, vector_store, user_description)\n",
    "                if prompt:\n",
    "                    description = generate_description(model, tokenizer, prompt)\n",
    "                    df.at[idx, 'Generated_Description'] = description\n",
    "                    df.at[idx, 'Source'] = 'Non-PDF'\n",
    "                    df.at[idx, 'Confidence_Score'] = 0.7\n",
    "        output_df = df[['ManufacturerId', 'MachineModelId', 'MachineTypeId', 'MachineConditionId',\n",
    "                         'MachineAvailabilityId', 'Location', 'Year', 'Description', \n",
    "                         'Generated_Description', 'Source', 'Confidence_Score']]\n",
    "        output_df.to_excel(output_file, index=False)\n",
    "        print(f\"Processing complete. Results saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Excel files: {str(e)}\")\n",
    "\n",
    "def clean_generated_text(text):\n",
    "    \"\"\"Enhanced cleaning of generated text.\"\"\"\n",
    "    if not text:\n",
    "        return \"Error: Empty generated text.\"\n",
    "    try:\n",
    "        text = re.sub(r'You are a technical writer.*?\\n', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'Create a detailed, professional description.*?Response must be in English only.', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', text)\n",
    "        if not text.endswith(('.', '!', '?')):\n",
    "            text = text.rstrip() + '.'\n",
    "        if not is_english(text):\n",
    "            translated = translate_to_english(text)\n",
    "            if translated and len(translated.split()) >= 3:\n",
    "                return translated\n",
    "            return \"Error: Unable to generate valid English description.\"\n",
    "        if len(text.split()) < 3:\n",
    "            return \"Error: Generated description too short.\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Text cleaning error: {str(e)}\")\n",
    "        return \"Error: Text cleaning failed.\"\n",
    "\n",
    "def generate_description(model, tokenizer, prompt, max_length=2048):\n",
    "    \"\"\"Generate description with improved parameters.\"\"\"\n",
    "    try:\n",
    "        if not all([prompt, model, tokenizer]):\n",
    "            return \"Insufficient data for description generation.\"\n",
    "        device = model.device\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, \n",
    "                           max_length=max_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_text = clean_generated_text(raw_text)\n",
    "        if len(cleaned_text.split()) < 50:\n",
    "            cleaned_text = generate_description_fallback(model, tokenizer, prompt, max_length)\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in description generation: {str(e)}\")\n",
    "        return \"Error generating description.\"\n",
    "\n",
    "def generate_description_fallback(model, tokenizer, prompt, max_length=2048):\n",
    "    \"\"\"Fallback function to generate a description if the initial attempt is too short.\"\"\"\n",
    "    try:\n",
    "        device = model.device\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, \n",
    "                           max_length=max_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                no_repeat_ngram_size=2,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_text = clean_generated_text(raw_text)\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in description generation fallback: {str(e)}\")\n",
    "        return \"Error generating description.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"/home/hbs/Downloads/MachineTbl_Data.xlsx\"\n",
    "output_file=\"/home/hbs/Downloads/descriptions.xlsx\"\n",
    "pdf_folder=\"/home/hbs/Downloads/PDFs\"\n",
    "manufacturer_mapping_file = \"/home/hbs/Downloads/manufacturer_mapping.csv\"\n",
    "model_mapping_file = \"/home/hbs/Downloads/model_mapping.xlsx\"\n",
    "user_description = \"This is a high-performance CNC machine with advanced controls and precision tooling. It can handle a wide range of materials and produce parts with exceptional accuracy.\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_excel(\n",
    "            input_file,\n",
    "            output_file,\n",
    "            manufacturer_mapping_file,\n",
    "            model_mapping_file,\n",
    "            pdf_folder,\n",
    "            user_description\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
